{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d404bb",
   "metadata": {},
   "source": [
    "# Heston Surrogate Pricer - Complete Pipeline\n",
    "\n",
    "## ðŸŽ¯ Project Overview\n",
    "\n",
    "This notebook demonstrates the complete training and evaluation pipeline for the **Heston surrogate pricing model**. Traditional option pricing under the Heston stochastic volatility model requires computationally expensive Fast Fourier Transform (FFT) methods. Our approach replaces this with a fast, accurate neural network that learns to predict implied volatility surfaces directly from Heston parameters.\n",
    "\n",
    "### ðŸ“š Theoretical Background\n",
    "\n",
    "The **Heston Model** (1993) describes asset price dynamics with stochastic volatility:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "dS_t &= rS_t dt + \\sqrt{v_t}S_t dW_1^t \\\\\n",
    "dv_t &= \\kappa(\\theta - v_t)dt + \\sigma\\sqrt{v_t}dW_2^t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S_t$: Asset price at time $t$\n",
    "- $v_t$: Instantaneous variance at time $t$\n",
    "- $r$: Risk-free rate\n",
    "- $\\kappa$: Mean reversion speed\n",
    "- $\\theta$: Long-term variance level\n",
    "- $\\sigma$: Volatility of volatility (vol-of-vol)\n",
    "- $\\rho = dW_1^t \\cdot dW_2^t$: Correlation between price and volatility shocks\n",
    "\n",
    "### ðŸš€ Surrogate Model Approach\n",
    "\n",
    "Instead of solving the complex pricing integral:\n",
    "$$C(K,T) = e^{-rT} \\mathbb{E}[\\max(S_T - K, 0)]$$\n",
    "\n",
    "We train a neural network to learn the direct mapping:\n",
    "$$f_{\\text{NN}}: (v_0, \\kappa, \\theta, \\sigma, \\rho, r, K, T) \\mapsto \\text{IV}(K,T)$$\n",
    "\n",
    "This provides **~1000x speedup** over traditional FFT methods while maintaining high accuracy.\n",
    "\n",
    "## ðŸ“‹ Pipeline Overview\n",
    "\n",
    "1. **ðŸ”§ Environment Setup**: Import libraries and configure reproducibility\n",
    "2. **âš™ï¸ Configuration**: Define hyperparameters and training settings\n",
    "3. **ðŸ“Š Data Loading**: Load preprocessed Heston parameter-IV surface pairs\n",
    "4. **ðŸŽ¯ PCA Analysis**: Reduce output dimensionality while preserving structure\n",
    "5. **ðŸ—ï¸ Model Architecture**: Build ResidualMLP with advanced loss function\n",
    "6. **ðŸš‚ Model Training**: Train with callbacks and monitoring\n",
    "7. **ðŸ“ˆ Training Analysis**: Visualize learning curves and convergence\n",
    "8. **ðŸ§ª Model Evaluation**: Comprehensive test set evaluation\n",
    "9. **ðŸŽ¯ Bucket Analysis**: Performance across different market regions\n",
    "10. **ðŸ“Š Visualization**: IV surface comparisons and error analysis\n",
    "11. **ðŸ” Statistical Analysis**: Residual distribution and normality tests\n",
    "12. **ðŸ’¾ Results Export**: Save artifacts and generate comprehensive reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b37b5c",
   "metadata": {},
   "source": [
    "## 1. ðŸ”§ Setup and Imports\n",
    "\n",
    "### ðŸ“š Library Dependencies\n",
    "\n",
    "We begin by importing all necessary libraries for our machine learning pipeline:\n",
    "\n",
    "- **Core Scientific Computing**: `numpy`, `scipy` for mathematical operations\n",
    "- **Data Analysis**: `pandas` for data manipulation\n",
    "- **Machine Learning**: `tensorflow`, `keras` for neural network implementation\n",
    "- **Dimensionality Reduction**: `sklearn.decomposition.PCA` for output compression\n",
    "- **Visualization**: `matplotlib`, `seaborn` for plotting and analysis\n",
    "- **Utilities**: `pickle`, `json` for serialization and configuration\n",
    "\n",
    "### ðŸŽ¯ Project Module Imports\n",
    "\n",
    "Our custom modules provide specialized functionality:\n",
    "\n",
    "- **`model_architectures`**: ResidualMLP implementation, PCA utilities, advanced loss functions\n",
    "- **`training_utils`**: Reproducibility setup, callback creation, artifact management\n",
    "\n",
    "### ðŸ”’ Reproducibility Setup\n",
    "\n",
    "Ensuring reproducible results is crucial for scientific validity. We'll set random seeds for:\n",
    "- **NumPy**: `np.random.seed()`\n",
    "- **TensorFlow**: `tf.random.set_seed()`\n",
    "- **Python**: `random.seed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.model_architectures import (\n",
    "    build_resmlp_pca_model,\n",
    "    fit_pca_components,\n",
    "    create_advanced_loss_function,\n",
    "    pca_transform_targets,\n",
    "    pca_inverse_transform\n",
    ")\n",
    "from src.training_utils import (\n",
    "    setup_reproducibility,\n",
    "    create_training_callbacks,\n",
    "    save_training_artifacts\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478919d",
   "metadata": {},
   "source": [
    "## 2. âš™ï¸ Configuration and Hyperparameters\n",
    "\n",
    "### ðŸŽ›ï¸ Hyperparameter Philosophy\n",
    "\n",
    "Proper hyperparameter selection is critical for model performance. Our configuration balances:\n",
    "\n",
    "1. **Model Capacity**: Sufficient parameters to capture complex IV surface relationships\n",
    "2. **Regularization**: Prevent overfitting through dropout and advanced loss functions\n",
    "3. **Training Efficiency**: Optimal batch sizes and learning rates for convergence\n",
    "4. **Computational Resources**: Memory and time constraints\n",
    "\n",
    "### ðŸ“Š Data Configuration\n",
    "\n",
    "- **`data_size`**: Dataset size (5k, 100k, 150k samples)\n",
    "- **`data_format`**: Storage format (`modular` for separate .npy files, `npz` for compressed)\n",
    "\n",
    "### ðŸ—ï¸ Architecture Configuration\n",
    "\n",
    "- **`pca_components`**: Number of principal components (typically 12-30)\n",
    "  - Reduces output dimensionality from 60 (10 strikes Ã— 6 tenors) to K components\n",
    "  - Preserves >99.9% of variance while improving training stability\n",
    "- **`n_blocks`**: Number of residual blocks (depth of network)\n",
    "- **`width`**: Hidden layer width (model capacity)\n",
    "- **`dropout_rate`**: Regularization strength (0.0-0.3 typical)\n",
    "\n",
    "### ðŸŽ¯ Advanced Loss Function Parameters\n",
    "\n",
    "Our loss function combines multiple objectives:\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{Huber}} + \\alpha L_{\\text{Sobolev}}^{(K)} + \\beta L_{\\text{Sobolev}}^{(T)} + W_{\\text{OTM}} \\cdot L_{\\text{weighted}}$$\n",
    "\n",
    "- **`huber_delta`**: Huber loss threshold (robust to outliers)\n",
    "- **`sobolev_alpha`**: Strike smoothness regularization weight\n",
    "- **`sobolev_beta`**: Tenor smoothness regularization weight  \n",
    "- **`otm_put_weight`**: Increased weight for challenging OTM Put region\n",
    "\n",
    "### ðŸš‚ Training Configuration\n",
    "\n",
    "- **`epochs`**: Maximum training iterations\n",
    "- **`batch_size`**: Mini-batch size (affects gradient noise and memory)\n",
    "- **`learning_rate`**: Initial optimizer learning rate\n",
    "- **`patience`**: Early stopping patience (prevent overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data\n",
    "    'data_size': '100k',\n",
    "    'data_format': 'modular',  # or 'npz'\n",
    "    \n",
    "    # Model Architecture\n",
    "    'pca_components': 30,\n",
    "    'n_blocks': 8,\n",
    "    'width': 128,\n",
    "    'dropout_rate': 0.1,\n",
    "    \n",
    "    # Loss Function\n",
    "    'huber_delta': 0.1,\n",
    "    'sobolev_alpha': 0.01,\n",
    "    'sobolev_beta': 0.01,\n",
    "    'otm_put_weight': 2.0,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 200,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 20,\n",
    "    \n",
    "    # Reproducibility\n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # Paths\n",
    "    'data_path': project_root / 'data' / 'raw' / f'data_{\"100k\"}',\n",
    "    'experiments_path': project_root / 'experiments',\n",
    "    'reports_path': project_root / 'reports',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40e62e",
   "metadata": {},
   "source": [
    "## 3. ðŸ“Š Data Loading and Preprocessing\n",
    "\n",
    "### ðŸ”¢ Dataset Structure\n",
    "\n",
    "Our dataset consists of parameter-IV surface pairs generated using FFT-based Heston pricing:\n",
    "\n",
    "- **Input Features (X)**: 15-dimensional parameter vectors\n",
    "  - Heston parameters: $(v_0, \\kappa, \\theta, \\sigma, \\rho)$\n",
    "  - Market conditions: $(r)$ (risk-free rate)\n",
    "  - Contract specifications: $(K_1, ..., K_{10}, T_1, ..., T_6)$ (strikes and tenors)\n",
    "\n",
    "- **Target Values (y)**: 60-dimensional IV vectors\n",
    "  - Implied volatility surface: $\\text{IV}(K_i, T_j)$ for $i=1...10, j=1...6$\n",
    "  - Represents the \"ground truth\" from expensive FFT calculations\n",
    "\n",
    "### ðŸŽ¯ Data Split Strategy\n",
    "\n",
    "We use a standard 60/20/20 train/validation/test split:\n",
    "\n",
    "- **Training Set**: Model parameter optimization\n",
    "- **Validation Set**: Hyperparameter tuning and early stopping\n",
    "- **Test Set**: Final unbiased performance evaluation\n",
    "\n",
    "### ðŸ”„ Data Normalization\n",
    "\n",
    "Proper scaling is essential for neural network training:\n",
    "\n",
    "- **Input Scaling**: StandardScaler (zero mean, unit variance)\n",
    "  $$X_{\\text{scaled}} = \\frac{X - \\mu_X}{\\sigma_X}$$\n",
    "\n",
    "- **Output Scaling**: MinMaxScaler (bounded range)\n",
    "  $$y_{\\text{scaled}} = \\frac{y - y_{\\text{min}}}{y_{\\text{max}} - y_{\\text{min}}}$$\n",
    "\n",
    "### ðŸ“ˆ Data Quality Insights\n",
    "\n",
    "We'll examine:\n",
    "- **Distribution characteristics**: Mean, variance, skewness\n",
    "- **Range analysis**: Min/max values for sanity checking\n",
    "- **Missing values**: Data completeness verification"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
